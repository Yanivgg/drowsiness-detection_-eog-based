{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Phase 2: ML Training for Drowsiness Detection\n",
        "\n",
        "**Train Random Forest & SVM on 16s EOG features**\n",
        "\n",
        "- Train: Subjects 01-06, 08-10\n",
        "- Test: Subject 07 (same as CNN Phase 1)\n",
        "- Window: 16 seconds, Stride: 8 seconds (50% overlap)\n",
        "- Features: 63 features (time, frequency, non-linear, EOG-specific)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Phase 2: ML Training for Drowsiness Detection\n",
        "\n",
        "**Train Random Forest & SVM on 16s EOG features**\n",
        "\n",
        "- Train: Subjects 01-06, 08-10\n",
        "- Test: Subject 07 (same as CNN Phase 1)\n",
        "- Window: 16 seconds, Stride: 8 seconds (50% overlap)\n",
        "- Features: 63 features (time, frequency, non-linear, EOG-specific)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup & Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score, f1_score,\n",
        "    confusion_matrix, classification_report, cohen_kappa_score\n",
        ")\n",
        "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
        "import joblib\n",
        "import scipy.io as spio\n",
        "from datetime import datetime\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "print(\"=\" * 70)\n",
        "print(\"Phase 2: ML Training for Drowsiness Detection\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"Started: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Mount Google Drive (for saving models)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "# Create results directory\n",
        "RESULTS_DIR = '/content/drive/MyDrive/drowsiness_ml_results'\n",
        "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
        "print(f\"\\n[OK] Results will be saved to: {RESULTS_DIR}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Load Feature Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"Loading Feature Data\")\n",
        "print(\"=\" * 70)\n",
        "# Load train and test features\n",
        "train_df = pd.read_csv('/content/train_features_16s.csv')\n",
        "test_df = pd.read_csv('/content/test_features_16s.csv')\n",
        "print(f\"\\n[OK] Train data: {len(train_df):,} windows\")\n",
        "print(f\"[OK] Test data: {len(test_df):,} windows\")\n",
        "# Separate metadata from features\n",
        "METADATA_COLS = ['Subject', 'Trial', 'Window', 'StartSample', 'EndSample', 'Label']\n",
        "FEATURE_COLS = [col for col in train_df.columns if col not in METADATA_COLS]\n",
        "print(f\"\\n[OK] Metadata columns: {len(METADATA_COLS)}\")\n",
        "print(f\"[OK] Feature columns: {len(FEATURE_COLS)}\")\n",
        "print(f\"\\nFeature types breakdown:\")\n",
        "print(f\"  - Time-domain: {len([f for f in FEATURE_COLS if f.startswith('td_')])}\")\n",
        "print(f\"  - Frequency-domain: {len([f for f in FEATURE_COLS if f.startswith('fd_')])}\")\n",
        "print(f\"  - Non-linear: {len([f for f in FEATURE_COLS if f.startswith('nl_')])}\")\n",
        "print(f\"  - EOG-specific: {len([f for f in FEATURE_COLS if f.startswith('eog_')])}\")\n",
        "# Prepare train/test splits\n",
        "X_train = train_df[FEATURE_COLS].values\n",
        "y_train = train_df['Label'].values\n",
        "X_test = test_df[FEATURE_COLS].values\n",
        "y_test = test_df['Label'].values\n",
        "print(f\"\\n[OK] X_train shape: {X_train.shape}\")\n",
        "print(f\"[OK] X_test shape: {X_test.shape}\")\n",
        "print(f\"[OK] y_train distribution: {np.bincount(y_train)} (0=awake, 1=drowsy)\")\n",
        "print(f\"[OK] y_test distribution: {np.bincount(y_test)} (0=awake, 1=drowsy)\")\n",
        "# Calculate class distribution\n",
        "train_drowsy_pct = 100 * np.sum(y_train == 1) / len(y_train)\n",
        "test_drowsy_pct = 100 * np.sum(y_test == 1) / len(y_test)\n",
        "print(f\"\\nðŸ“Š Train drowsy: {train_drowsy_pct:.2f}%\")\n",
        "print(f\"ðŸ“Š Test drowsy: {test_drowsy_pct:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Train Random Forest Classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"Training Random Forest Classifier\")\n",
        "print(\"=\" * 70)\n",
        "# Train with class balancing\n",
        "rf_model = RandomForestClassifier(\n",
        "    n_estimators=200,\n",
        "    max_depth=20,\n",
        "    min_samples_split=10,\n",
        "    min_samples_leaf=5,\n",
        "    class_weight='balanced',  # Handle class imbalance\n",
        "    random_state=42,\n",
        "    n_jobs=-1,\n",
        "    verbose=1\n",
        ")\n",
        "print(\"\\n[OK] Training Random Forest...\")\n",
        "rf_model.fit(X_train, y_train)\n",
        "print(\"[OK] Training completed!\")\n",
        "# Evaluate on test set\n",
        "y_pred_rf = rf_model.predict(X_test)\n",
        "y_proba_rf = rf_model.predict_proba(X_test)[:, 1]\n",
        "# Calculate metrics\n",
        "rf_metrics = {\n",
        "    'accuracy': accuracy_score(y_test, y_pred_rf),\n",
        "    'precision': precision_score(y_test, y_pred_rf),\n",
        "    'recall': recall_score(y_test, y_pred_rf),\n",
        "    'f1': f1_score(y_test, y_pred_rf),\n",
        "    'kappa': cohen_kappa_score(y_test, y_pred_rf),\n",
        "    'confusion_matrix': confusion_matrix(y_test, y_pred_rf)\n",
        "}\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"Random Forest Results on Subject 07 Test Set\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"\\nðŸ“Š Performance Metrics:\")\n",
        "print(f\"  Accuracy:       {rf_metrics['accuracy']:.4f} ({rf_metrics['accuracy']*100:.2f}%)\")\n",
        "print(f\"  F1-score:       {rf_metrics['f1']:.4f}\")\n",
        "print(f\"  Cohen's Kappa:  {rf_metrics['kappa']:.4f}\")\n",
        "print(f\"\\nðŸŽ¯ Drowsy Detection (Class 1):\")\n",
        "print(f\"  Precision: {rf_metrics['precision']:.4f} ({rf_metrics['precision']*100:.2f}%)\")\n",
        "print(f\"  Recall:    {rf_metrics['recall']:.4f} ({rf_metrics['recall']*100:.2f}%)\")\n",
        "print(f\"\\nðŸ“‰ Confusion Matrix:\")\n",
        "print(\"                 Predicted\")\n",
        "print(\"                 Awake    Drowsy\")\n",
        "cm = rf_metrics['confusion_matrix']\n",
        "print(f\"Actual  Awake   {cm[0,0]:,}  {cm[0,1]:,}\")\n",
        "print(f\"        Drowsy  {cm[1,0]:,}  {cm[1,1]:,}\")\n",
        "# Save model\n",
        "rf_model_path = os.path.join(RESULTS_DIR, 'random_forest_model.joblib')\n",
        "joblib.dump(rf_model, rf_model_path)\n",
        "print(f\"\\nâœ“ Model saved to: {rf_model_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Feature Importance Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"Feature Importance Analysis\")\n",
        "print(\"=\" * 70)\n",
        "# Get feature importances\n",
        "feature_importance = pd.DataFrame({\n",
        "    'feature': FEATURE_COLS,\n",
        "    'importance': rf_model.feature_importances_\n",
        "}).sort_values('importance', ascending=False)\n",
        "print(\"\\nðŸ“Š Top 20 Most Important Features:\")\n",
        "print(feature_importance.head(20).to_string(index=False))\n",
        "# Save feature importance\n",
        "importance_path = os.path.join(RESULTS_DIR, 'feature_importance.csv')\n",
        "feature_importance.to_csv(importance_path, index=False)\n",
        "print(f\"\\nâœ“ Feature importance saved to: {importance_path}\")\n",
        "# Plot top 20 features\n",
        "plt.figure(figsize=(12, 8))\n",
        "top_20 = feature_importance.head(20)\n",
        "plt.barh(range(len(top_20)), top_20['importance'])\n",
        "plt.yticks(range(len(top_20)), top_20['feature'])\n",
        "plt.xlabel('Importance')\n",
        "plt.title('Top 20 Most Important Features')\n",
        "plt.gca().invert_yaxis()\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(RESULTS_DIR, 'feature_importance_plot.png'), dpi=150)\n",
        "plt.show()\n",
        "print(\"\\n[OK] Feature importance plot saved\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Train SVM Classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"Training SVM Classifier\")\n",
        "print(\"=\" * 70)\n",
        "# Train SVM with class balancing\n",
        "svm_model = SVC(\n",
        "    kernel='rbf',\n",
        "    C=1.0,\n",
        "    gamma='scale',\n",
        "    class_weight='balanced',\n",
        "    random_state=42,\n",
        "    probability=True,  # Enable probability estimates\n",
        "    verbose=True\n",
        ")\n",
        "print(\"\\n[OK] Training SVM...\")\n",
        "svm_model.fit(X_train, y_train)\n",
        "print(\"[OK] Training completed!\")\n",
        "# Evaluate on test set\n",
        "y_pred_svm = svm_model.predict(X_test)\n",
        "y_proba_svm = svm_model.predict_proba(X_test)[:, 1]\n",
        "# Calculate metrics\n",
        "svm_metrics = {\n",
        "    'accuracy': accuracy_score(y_test, y_pred_svm),\n",
        "    'precision': precision_score(y_test, y_pred_svm),\n",
        "    'recall': recall_score(y_test, y_pred_svm),\n",
        "    'f1': f1_score(y_test, y_pred_svm),\n",
        "    'kappa': cohen_kappa_score(y_test, y_pred_svm),\n",
        "    'confusion_matrix': confusion_matrix(y_test, y_pred_svm)\n",
        "}\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"SVM Results on Subject 07 Test Set\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"\\nðŸ“Š Performance Metrics:\")\n",
        "print(f\"  Accuracy:       {svm_metrics['accuracy']:.4f} ({svm_metrics['accuracy']*100:.2f}%)\")\n",
        "print(f\"  F1-score:       {svm_metrics['f1']:.4f}\")\n",
        "print(f\"  Cohen's Kappa:  {svm_metrics['kappa']:.4f}\")\n",
        "print(f\"\\nðŸŽ¯ Drowsy Detection (Class 1):\")\n",
        "print(f\"  Precision: {svm_metrics['precision']:.4f} ({svm_metrics['precision']*100:.2f}%)\")\n",
        "print(f\"  Recall:    {svm_metrics['recall']:.4f} ({svm_metrics['recall']*100:.2f}%)\")\n",
        "print(f\"\\nðŸ“‰ Confusion Matrix:\")\n",
        "print(\"                 Predicted\")\n",
        "print(\"                 Awake    Drowsy\")\n",
        "cm = svm_metrics['confusion_matrix']\n",
        "print(f\"Actual  Awake   {cm[0,0]:,}  {cm[0,1]:,}\")\n",
        "print(f\"        Drowsy  {cm[1,0]:,}  {cm[1,1]:,}\")\n",
        "# Save model\n",
        "svm_model_path = os.path.join(RESULTS_DIR, 'svm_model.joblib')\n",
        "joblib.dump(svm_model, svm_model_path)\n",
        "print(f\"\\nâœ“ Model saved to: {svm_model_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Model Comparison"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"Model Comparison on Subject 07 Test Set\")\n",
        "print(\"=\" * 70)\n",
        "comparison_df = pd.DataFrame({\n",
        "    'Model': ['Random Forest', 'SVM'],\n",
        "    'Accuracy': [rf_metrics['accuracy'], svm_metrics['accuracy']],\n",
        "    'Precision': [rf_metrics['precision'], svm_metrics['precision']],\n",
        "    'Recall': [rf_metrics['recall'], svm_metrics['recall']],\n",
        "    'F1-Score': [rf_metrics['f1'], svm_metrics['f1']],\n",
        "    'Cohen\\'s Kappa': [rf_metrics['kappa'], svm_metrics['kappa']]\n",
        "})\n",
        "print(\"\\n\" + comparison_df.to_string(index=False))\n",
        "# Save comparison\n",
        "comparison_path = os.path.join(RESULTS_DIR, 'model_comparison.csv')\n",
        "comparison_df.to_csv(comparison_path, index=False)\n",
        "print(f\"\\nâœ“ Comparison saved to: {comparison_path}\")\n",
        "# Plot comparison\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "# Metrics comparison\n",
        "metrics_to_plot = ['Precision', 'Recall', 'F1-Score', 'Cohen\\'s Kappa']\n",
        "x = np.arange(len(metrics_to_plot))\n",
        "width = 0.35\n",
        "axes[0].bar(x - width/2, comparison_df[metrics_to_plot].iloc[0], width, label='Random Forest', alpha=0.8)\n",
        "axes[0].bar(x + width/2, comparison_df[metrics_to_plot].iloc[1], width, label='SVM', alpha=0.8)\n",
        "axes[0].set_xlabel('Metrics')\n",
        "axes[0].set_ylabel('Score')\n",
        "axes[0].set_title('Model Performance Comparison')\n",
        "axes[0].set_xticks(x)\n",
        "axes[0].set_xticklabels(metrics_to_plot, rotation=45, ha='right')\n",
        "axes[0].legend()\n",
        "axes[0].grid(axis='y', alpha=0.3)\n",
        "axes[0].set_ylim(0, 1)\n",
        "# Confusion matrices side by side\n",
        "cm_rf = rf_metrics['confusion_matrix']\n",
        "cm_svm = svm_metrics['confusion_matrix']\n",
        "# Normalize for better visualization\n",
        "cm_rf_norm = cm_rf.astype('float') / cm_rf.sum(axis=1)[:, np.newaxis]\n",
        "cm_svm_norm = cm_svm.astype('float') / cm_svm.sum(axis=1)[:, np.newaxis]\n",
        "im1 = axes[1].imshow(cm_rf_norm, interpolation='nearest', cmap=plt.cm.Blues)\n",
        "axes[1].set_title('Random Forest\\nNormalized Confusion Matrix')\n",
        "axes[1].set_xlabel('Predicted')\n",
        "axes[1].set_ylabel('Actual')\n",
        "axes[1].set_xticks([0, 1])\n",
        "axes[1].set_yticks([0, 1])\n",
        "axes[1].set_xticklabels(['Awake', 'Drowsy'])\n",
        "axes[1].set_yticklabels(['Awake', 'Drowsy'])\n",
        "# Add text annotations\n",
        "for i in range(2):\n",
        "    for j in range(2):\n",
        "        text = axes[1].text(j, i, f'{cm_rf_norm[i, j]:.2f}\\n({cm_rf[i, j]:,})',\n",
        "                           ha=\"center\", va=\"center\", color=\"white\" if cm_rf_norm[i, j] > 0.5 else \"black\")\n",
        "plt.colorbar(im1, ax=axes[1])\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(RESULTS_DIR, 'model_comparison_plot.png'), dpi=150)\n",
        "plt.show()\n",
        "print(\"\\n[OK] Comparison plots saved\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Save Complete Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"Saving Complete Results\")\n",
        "print(\"=\" * 70)\n",
        "# Save predictions\n",
        "predictions_rf = pd.DataFrame({\n",
        "    'Subject': test_df['Subject'],\n",
        "    'Trial': test_df['Trial'],\n",
        "    'Window': test_df['Window'],\n",
        "    'True_Label': y_test,\n",
        "    'RF_Prediction': y_pred_rf,\n",
        "    'RF_Probability': y_proba_rf,\n",
        "    'SVM_Prediction': y_pred_svm,\n",
        "    'SVM_Probability': y_proba_svm\n",
        "})\n",
        "predictions_path = os.path.join(RESULTS_DIR, 'predictions_subject07.csv')\n",
        "predictions_rf.to_csv(predictions_path, index=False)\n",
        "print(f\"[OK] Predictions saved to: {predictions_path}\")\n",
        "# Save all metrics to .mat file for consistency with Phase 1\n",
        "results_mat = {\n",
        "    'rf_metrics': {\n",
        "        'accuracy': rf_metrics['accuracy'],\n",
        "        'precision': rf_metrics['precision'],\n",
        "        'recall': rf_metrics['recall'],\n",
        "        'f1': rf_metrics['f1'],\n",
        "        'kappa': rf_metrics['kappa'],\n",
        "        'confusion_matrix': rf_metrics['confusion_matrix']\n",
        "    },\n",
        "    'svm_metrics': {\n",
        "        'accuracy': svm_metrics['accuracy'],\n",
        "        'precision': svm_metrics['precision'],\n",
        "        'recall': svm_metrics['recall'],\n",
        "        'f1': svm_metrics['f1'],\n",
        "        'kappa': svm_metrics['kappa'],\n",
        "        'confusion_matrix': svm_metrics['confusion_matrix']\n",
        "    },\n",
        "    'test_info': {\n",
        "        'subject': '07F',\n",
        "        'n_windows': len(y_test),\n",
        "        'n_drowsy': int(np.sum(y_test == 1)),\n",
        "        'n_awake': int(np.sum(y_test == 0))\n",
        "    }\n",
        "}\n",
        "results_mat_path = os.path.join(RESULTS_DIR, 'ml_test_results.mat')\n",
        "spio.savemat(results_mat_path, results_mat)\n",
        "print(f\"[OK] Results .mat file saved to: {results_mat_path}\")\n",
        "# Create summary report\n",
        "summary_report = f\"\"\"\n",
        "================================================================================\n",
        "Phase 2: ML Training Results Summary\n",
        "================================================================================\n",
        "Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
        "DATASET INFO:\n",
        "- Train: Subjects 01-06, 08-10 ({len(X_train):,} windows, {train_drowsy_pct:.2f}% drowsy)\n",
        "- Test: Subject 07 ({len(X_test):,} windows, {test_drowsy_pct:.2f}% drowsy)\n",
        "- Window: 16 seconds (3200 samples)\n",
        "- Stride: 8 seconds (50% overlap)\n",
        "- Features: {len(FEATURE_COLS)} features\n",
        "RANDOM FOREST RESULTS:\n",
        "- Accuracy:  {rf_metrics['accuracy']:.4f} ({rf_metrics['accuracy']*100:.2f}%)\n",
        "- Precision: {rf_metrics['precision']:.4f} ({rf_metrics['precision']*100:.2f}%)\n",
        "- Recall:    {rf_metrics['recall']:.4f} ({rf_metrics['recall']*100:.2f}%)\n",
        "- F1-Score:  {rf_metrics['f1']:.4f}\n",
        "- Kappa:     {rf_metrics['kappa']:.4f}\n",
        "SVM RESULTS:\n",
        "- Accuracy:  {svm_metrics['accuracy']:.4f} ({svm_metrics['accuracy']*100:.2f}%)\n",
        "- Precision: {svm_metrics['precision']:.4f} ({svm_metrics['precision']*100:.2f}%)\n",
        "- Recall:    {svm_metrics['recall']:.4f} ({svm_metrics['recall']*100:.2f}%)\n",
        "- F1-Score:  {svm_metrics['f1']:.4f}\n",
        "- Kappa:     {svm_metrics['kappa']:.4f}\n",
        "FILES SAVED:\n",
        "- Random Forest model: random_forest_model.joblib\n",
        "- SVM model: svm_model.joblib\n",
        "- Feature importance: feature_importance.csv\n",
        "- Model comparison: model_comparison.csv\n",
        "- Predictions: predictions_subject07.csv\n",
        "- Results .mat: ml_test_results.mat\n",
        "- Plots: feature_importance_plot.png, model_comparison_plot.png\n",
        "================================================================================\n",
        "\"\"\"\n",
        "summary_path = os.path.join(RESULTS_DIR, 'training_summary.txt')\n",
        "with open(summary_path, 'w') as f:\n",
        "    f.write(summary_report)\n",
        "print(summary_report)\n",
        "print(f\"âœ“ Summary report saved to: {summary_path}\")\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"âœ… TRAINING COMPLETED SUCCESSFULLY!\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"\\nAll results saved to: {RESULTS_DIR}\")\n",
        "print(f\"Finished: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
