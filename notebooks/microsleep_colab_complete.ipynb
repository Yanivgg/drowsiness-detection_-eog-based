{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "header"
      },
      "source": [
        "# ðŸ§  Microsleep Detection Pipeline - Complete Workflow\n",
        "\n",
        "This notebook contains the complete pipeline for training and evaluating microsleep detection models.\n",
        "\n",
        "**Dataset:** Dryad EOG signals (2 channels: LOC-Ref, ROC-Ref)  \n",
        "**Task:** Binary classification (Awake vs. Drowsy)  \n",
        "**Models:** CNN (2s/4s/8s/16s/32s) and CNN-LSTM  \n",
        "**Test Subject:** 07 (100,600 microsleep samples - 23x more than Subject 10!)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "setup-header"
      },
      "source": [
        "## ðŸ“¦ Setup and Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "setup"
      },
      "outputs": [],
      "source": [
        "# Install required packages (if needed)\n",
        "# !pip install tensorflow keras scipy scikit-learn\n",
        "\n",
        "# Import libraries\n",
        "import os\n",
        "import sys\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"ðŸš€ Microsleep Detection Pipeline\")\n",
        "print(\"=\"*80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "config-header"
      },
      "source": [
        "## ðŸ”§ Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "config"
      },
      "outputs": [],
      "source": [
        "# ========================================\n",
        "# CONFIGURATION - MODIFY AS NEEDED\n",
        "# ========================================\n",
        "\n",
        "# Base path (choose one)\n",
        "# Option 1: Files uploaded directly to Colab\n",
        "BASE_PATH = '/content'\n",
        "\n",
        "# Option 2: Files mounted from Google Drive (uncomment to use)\n",
        "# BASE_PATH = '/content/drive/MyDrive/microsleep_cursor'\n",
        "\n",
        "# Model to train (choose one)\n",
        "# Options: 'CNN_2s', 'CNN_4s', 'CNN_8s', 'CNN_16s', 'CNN_32s', 'CNN_LSTM'\n",
        "MODEL_TO_TRAIN = 'CNN_8s'\n",
        "\n",
        "# Training parameters\n",
        "EPOCHS = 3\n",
        "BATCH_SIZE = 800  # Increase for faster training (requires more GPU memory)\n",
        "STRIDE = 1  # prec parameter (1=max augmentation, higher=faster but less data)\n",
        "\n",
        "# Results directory\n",
        "RESULTS_PATH = '/content/drive/MyDrive/microsleep_results'\n",
        "\n",
        "# Save models to Drive?\n",
        "SAVE_TO_DRIVE = True\n",
        "\n",
        "print(f\"âœ“ Configuration loaded\")\n",
        "print(f\"  Base path: {BASE_PATH}\")\n",
        "print(f\"  Model: {MODEL_TO_TRAIN}\")\n",
        "print(f\"  Epochs: {EPOCHS}\")\n",
        "print(f\"  Batch size: {BATCH_SIZE}\")\n",
        "print(f\"  Stride: {STRIDE}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mount-header"
      },
      "source": [
        "## ðŸ“ Mount Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mount"
      },
      "outputs": [],
      "source": [
        "# Mount Google Drive to save results\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "print(\"âœ“ Google Drive mounted\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "verify-header"
      },
      "source": [
        "## ðŸ“‚ Verify Files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "verify"
      },
      "outputs": [],
      "source": [
        "print(\"=\"*80)\n",
        "print(\"Checking files...\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Check data files\n",
        "data_files_path = f'{BASE_PATH}/data/files'\n",
        "file_sets_path = f'{BASE_PATH}/data/file_sets.mat'\n",
        "\n",
        "if os.path.exists(data_files_path):\n",
        "    mat_files = [f for f in os.listdir(data_files_path) if f.endswith('.mat')]\n",
        "    print(f\"âœ“ Found {len(mat_files)} .mat files in data/files/\")\n",
        "else:\n",
        "    print(f\"âŒ data/files/ not found at {data_files_path}\")\n",
        "\n",
        "if os.path.exists(file_sets_path):\n",
        "    import scipy.io as spio\n",
        "    fs = spio.loadmat(file_sets_path)\n",
        "    train_files = [f[0] for f in fs['files_train'].flatten()]\n",
        "    val_files = [f[0] for f in fs['files_val'].flatten()]\n",
        "    test_files = [f[0] for f in fs['files_test'].flatten()]\n",
        "    print(f\"âœ“ Found file_sets.mat\")\n",
        "    print(f\"  Train: {len(train_files)} files (subjects 01-06)\")\n",
        "    print(f\"  Val:   {len(val_files)} files (subjects 08-10)\")\n",
        "    print(f\"  Test:  {len(test_files)} files (subject 07 - most microsleep events)\")\n",
        "else:\n",
        "    print(f\"âŒ file_sets.mat not found\")\n",
        "\n",
        "# Check model directory\n",
        "if MODEL_TO_TRAIN == 'CNN_LSTM':\n",
        "    model_dir = f'{BASE_PATH}/code/CNN_LSTM'\n",
        "else:\n",
        "    model_dir = f'{BASE_PATH}/code/CNN/{MODEL_TO_TRAIN}'\n",
        "\n",
        "if os.path.exists(model_dir):\n",
        "    print(f\"âœ“ Model directory found: {model_dir}\")\n",
        "else:\n",
        "    print(f\"âŒ Model directory not found: {model_dir}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gpu-header"
      },
      "source": [
        "## ðŸŽ® Check GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gpu"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"GPU Check\")\n",
        "print(\"=\"*80)\n",
        "print(f\"TensorFlow version: {tf.__version__}\")\n",
        "\n",
        "gpus = tf.config.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "    print(f\"âœ“ GPU available: {len(gpus)} device(s)\")\n",
        "    for i, gpu in enumerate(gpus):\n",
        "        print(f\"  GPU {i}: {gpu}\")\n",
        "else:\n",
        "    print(\"âš ï¸ No GPU found - training will be slow!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "update-params-header"
      },
      "source": [
        "## âš™ï¸ Update Training Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "update-params"
      },
      "outputs": [],
      "source": [
        "# Update batch_size, epochs, and stride in train.py\n",
        "\n",
        "os.chdir(model_dir)\n",
        "print(f\"Working directory: {os.getcwd()}\")\n",
        "\n",
        "# Read train.py\n",
        "with open('train.py', 'r', encoding='utf-8') as f:\n",
        "    train_content = f.read()\n",
        "\n",
        "# Update batch_size\n",
        "if 'batch_size = ' in train_content:\n",
        "    import re\n",
        "    train_content = re.sub(r'batch_size = \\d+', f'batch_size = {BATCH_SIZE}', train_content)\n",
        "    print(f\"âœ“ Updated batch_size to {BATCH_SIZE}\")\n",
        "\n",
        "# Update prec (stride)\n",
        "if 'prec = ' in train_content:\n",
        "    train_content = re.sub(r'prec = \\d+', f'prec = {STRIDE}', train_content)\n",
        "    print(f\"âœ“ Updated stride (prec) to {STRIDE}\")\n",
        "\n",
        "# Update n_epochs\n",
        "if 'n_epochs = ' in train_content:\n",
        "    train_content = re.sub(r'n_epochs = \\d+', f'n_epochs = {EPOCHS}', train_content)\n",
        "    print(f\"âœ“ Updated epochs to {EPOCHS}\")\n",
        "\n",
        "# Write back\n",
        "with open('train.py', 'w', encoding='utf-8') as f:\n",
        "    f.write(train_content)\n",
        "\n",
        "print(\"âœ“ Training parameters updated\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "train-header"
      },
      "source": [
        "## ðŸ‹ï¸ Train Model\n",
        "\n",
        "**Note:** This will take 3-4 hours depending on GPU and model size."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "train"
      },
      "outputs": [],
      "source": [
        "print(\"=\"*80)\n",
        "print(f\"Starting training: {MODEL_TO_TRAIN}\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Run training\n",
        "exec(open('train.py').read())\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"âœ“ Training completed!\")\n",
        "print(\"=\"*80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "save-header"
      },
      "source": [
        "## ðŸ’¾ Save Model to Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "save"
      },
      "outputs": [],
      "source": [
        "if SAVE_TO_DRIVE:\n",
        "    print(\"=\"*80)\n",
        "    print(\"Saving model to Google Drive...\")\n",
        "    print(\"=\"*80)\n",
        "    \n",
        "    # Create results directory if needed\n",
        "    os.makedirs(RESULTS_PATH, exist_ok=True)\n",
        "    \n",
        "    # Find the latest model\n",
        "    model_files = []\n",
        "    if os.path.exists('./models'):\n",
        "        model_files = [f for f in os.listdir('./models') if f.endswith('.h5')]\n",
        "    if os.path.exists('./model.h5'):\n",
        "        model_files.append('model.h5')\n",
        "    \n",
        "    if model_files:\n",
        "        # Copy model to Drive\n",
        "        if os.path.exists('./model.h5'):\n",
        "            src = './model.h5'\n",
        "        elif model_files:\n",
        "            # Get latest epoch model\n",
        "            model_files_full = [f'./models/{f}' for f in model_files if 'models/' not in f]\n",
        "            src = max(model_files_full, key=os.path.getmtime) if model_files_full else f'./models/{model_files[0]}'\n",
        "        \n",
        "        dst = f'{RESULTS_PATH}/{MODEL_TO_TRAIN}_model.h5'\n",
        "        shutil.copy(src, dst)\n",
        "        print(f\"âœ“ Model saved to: {dst}\")\n",
        "        \n",
        "        # Also save to local model.h5 for predict scripts\n",
        "        if not os.path.exists('./model.h5'):\n",
        "            shutil.copy(src, './model.h5')\n",
        "            print(f\"âœ“ Model copied to ./model.h5\")\n",
        "    else:\n",
        "        print(\"âš ï¸ No model file found to save\")\n",
        "else:\n",
        "    print(\"Model saving to Drive skipped (SAVE_TO_DRIVE=False)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "validate-header"
      },
      "source": [
        "## ðŸ“Š Validate on Validation Set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "validate"
      },
      "outputs": [],
      "source": [
        "print(\"=\"*80)\n",
        "print(\"Running validation...\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Run validation\n",
        "exec(open('predict_val.py').read())\n",
        "\n",
        "print(\"\\nâœ“ Validation completed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "test-header"
      },
      "source": [
        "## ðŸ§ª Predict on Test Set\n",
        "\n",
        "**Test Set:** Subject 07 with 100,600 microsleep samples (23x more than Subject 10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "test"
      },
      "outputs": [],
      "source": [
        "print(\"=\"*80)\n",
        "print(\"Running predictions on test set...\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Run prediction\n",
        "exec(open('predict.py').read())\n",
        "\n",
        "print(\"\\nâœ“ Test predictions completed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "analyze-header"
      },
      "source": [
        "## ðŸ“ˆ Analyze Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "analyze"
      },
      "outputs": [],
      "source": [
        "import scipy.io as spio\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, cohen_kappa_score\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(f\"TEST SET RESULTS - {MODEL_TO_TRAIN}\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Load predictions\n",
        "pred_file = './predictions_output.mat'\n",
        "data = spio.loadmat(pred_file)\n",
        "\n",
        "y_true = data['y_true'].flatten()\n",
        "y_pred = data['y_pred'].flatten()\n",
        "\n",
        "# Calculate metrics\n",
        "accuracy = accuracy_score(y_true, y_pred)\n",
        "f1 = f1_score(y_true, y_pred, average='weighted')\n",
        "kappa = cohen_kappa_score(y_true, y_pred)\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "# Per-class metrics\n",
        "if cm[1,0] + cm[1,1] > 0:\n",
        "    recall_drowsy = cm[1,1] / (cm[1,0] + cm[1,1])\n",
        "    precision_drowsy = cm[1,1] / (cm[0,1] + cm[1,1]) if (cm[0,1] + cm[1,1]) > 0 else 0\n",
        "else:\n",
        "    recall_drowsy = 0\n",
        "    precision_drowsy = 0\n",
        "\n",
        "print(f\"\\nðŸ“Š Performance Metrics:\")\n",
        "print(f\"  Accuracy:       {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
        "print(f\"  F1-score:       {f1:.4f}\")\n",
        "print(f\"  Cohen's Kappa:  {kappa:.4f}\")\n",
        "\n",
        "print(f\"\\nðŸŽ¯ Microsleep Detection (Class 1):\")\n",
        "print(f\"  Precision: {precision_drowsy:.4f} ({precision_drowsy*100:.2f}%)\")\n",
        "print(f\"  Recall:    {recall_drowsy:.4f} ({recall_drowsy*100:.2f}%)\")\n",
        "\n",
        "print(f\"\\nðŸ“‰ Confusion Matrix:\")\n",
        "print(f\"                 Predicted\")\n",
        "print(f\"                 Awake    Drowsy\")\n",
        "print(f\"Actual  Awake   {cm[0,0]:8,}  {cm[0,1]:6,}\")\n",
        "print(f\"        Drowsy  {cm[1,0]:8,}  {cm[1,1]:6,}\")\n",
        "\n",
        "# Class distribution\n",
        "n_awake = np.sum(y_true == 0)\n",
        "n_drowsy = np.sum(y_true == 1)\n",
        "print(f\"\\nðŸ“Š Class Distribution:\")\n",
        "print(f\"  Awake:  {n_awake:,} ({n_awake/len(y_true)*100:.2f}%)\")\n",
        "print(f\"  Drowsy: {n_drowsy:,} ({n_drowsy/len(y_true)*100:.2f}%)\")\n",
        "\n",
        "# Save summary\n",
        "if SAVE_TO_DRIVE:\n",
        "    summary = {\n",
        "        'model': MODEL_TO_TRAIN,\n",
        "        'accuracy': accuracy,\n",
        "        'f1_score': f1,\n",
        "        'kappa': kappa,\n",
        "        'recall_drowsy': recall_drowsy,\n",
        "        'precision_drowsy': precision_drowsy,\n",
        "        'confusion_matrix': cm,\n",
        "        'y_true': y_true,\n",
        "        'y_pred': y_pred\n",
        "    }\n",
        "    summary_path = f'{RESULTS_PATH}/{MODEL_TO_TRAIN}_test_results.mat'\n",
        "    spio.savemat(summary_path, summary)\n",
        "    print(f\"\\nâœ“ Results saved to: {summary_path}\")\n",
        "\n",
        "print(\"=\"*80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "keepalive-header"
      },
      "source": [
        "## ðŸ”„ Keep Colab Alive (Optional)\n",
        "\n",
        "Run this cell to prevent Colab from disconnecting due to inactivity during long training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "keepalive"
      },
      "outputs": [],
      "source": [
        "from IPython.display import display, HTML\n",
        "\n",
        "js_code = \"\"\"\n",
        "<script>\n",
        "function KeepClicking(){\n",
        "    console.log(\"Keeping Colab alive...\");\n",
        "    document.querySelector(\"colab-connect-button\").click();\n",
        "}\n",
        "setInterval(KeepClicking, 60000);  // Click every 60 seconds\n",
        "</script>\n",
        "\"\"\"\n",
        "\n",
        "display(HTML(js_code))\n",
        "print(\"âœ“ Keep-alive script activated\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "complete-header"
      },
      "source": [
        "## ðŸŽ‰ Pipeline Complete!\n",
        "\n",
        "### Next Steps:\n",
        "\n",
        "1. **Train other models:** Change `MODEL_TO_TRAIN` in Configuration cell and rerun from there\n",
        "2. **Compare models:** Use the saved results in `microsleep_results/`\n",
        "3. **Tune hyperparameters:** Adjust `BATCH_SIZE`, `STRIDE`, or model architecture\n",
        "4. **Analyze failures:** Look at cases where the model incorrectly predicts microsleep\n",
        "\n",
        "### Saved Files:\n",
        "- **Model:** `/content/drive/MyDrive/microsleep_results/{MODEL_NAME}_model.h5`\n",
        "- **Results:** `/content/drive/MyDrive/microsleep_results/{MODEL_NAME}_test_results.mat`\n",
        "- **Predictions:** `{model_dir}/predictions_output.mat`\n",
        "\n",
        "### Test Set Info:\n",
        "- **Subject 07** selected as test set\n",
        "- **100,600 microsleep samples** (3.45% of data)\n",
        "- **23x more** microsleep events than Subject 10\n",
        "- More statistically significant and reliable evaluation!"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
